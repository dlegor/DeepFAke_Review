{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"toc\"></a>\n# Table of Contents\n1. [Introduction](#introduction)\n1. [Configure hyper-parameters](#configure_hyper_parameters)\n1. [Install dependencies](#install_dependencies)\n1. [Import libraries](#import_libraries)\n1. [Define useful classes](#define_useful_classes)\n1. [Define helper-functions](#define_helper_functions)\n1. [Start inference process](#start_inference_process)\n1. [Create submission.csv](#create_submission_csv)\n1. [Conclusion](#conclusion)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"introduction\"></a>\n# Introduction\nLet's use our trained Logistic Regression model in the previous step to infer all test videos, and submit the result to see how our simple model perform :)\n\n---\nBaseline's diagram\n![diagram](https://drive.google.com/uc?id=1vfKBChg33xHE1v5hRr-fP3ixxtlQjDc4&export=download)\n\n---\nThis end-to-end solution includes 3 steps:\n1. [*Data Preparation* ](https://www.kaggle.com/phunghieu/deepfake-detection-data-preparation-baseline)\n1. [*Training*](https://www.kaggle.com/phunghieu/deepfake-detection-training-baseline)\n1. *Inference* <- **you're here**\n\n[Back to Table of Contents](#toc)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"configure_hyper_parameters\"></a>\n# Configure hyper-parameters\n[Back to Table of Contents](#toc)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"TEST_DIR = '/kaggle/input/deepfake-detection-challenge/test_videos/'\nMODEL_PATH = '/kaggle/input/deepfake-detection-logistic-regression/model.pth'\n\nBATCH_SIZE = 60\nSCALE = 0.25\nN_FRAMES = 10\nDEFAULT_PROB = 0.5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"install_dependencies\"></a>\n# Install dependencies\n[Back to Table of Contents](#toc)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Install facenet-pytorch\n!pip install /kaggle/input/facenet-pytorch-vggface2/facenet_pytorch-2.0.0-py3-none-any.whl\n\nfrom facenet_pytorch.models.inception_resnet_v1 import get_torch_home\ntorch_home = get_torch_home()\n\n# Copy model checkpoints to torch cache so they are loaded automatically by the package\n!mkdir -p $torch_home/checkpoints/\n!cp /kaggle/input/facenet-pytorch-vggface2/20180402-114759-vggface2-logits.pth $torch_home/checkpoints/vggface2_DG3kwML46X.pt\n!cp /kaggle/input/facenet-pytorch-vggface2/20180402-114759-vggface2-features.pth $torch_home/checkpoints/vggface2_G5aNV2VSMn.pt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"import_libraries\"></a>\n# Import libraries\n[Back to Table of Contents](#toc)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport glob\nimport json\nimport torch\nimport torch.nn as nn\nimport cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n# See github.com/timesler/facenet-pytorch:\nfrom facenet_pytorch import MTCNN, InceptionResnetV1, extract_face\n\nif torch.cuda.is_available():\n    device = 'cuda:0'\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\nelse:\n    device = 'cpu'\nprint(f'Running on device: {device}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"define_useful_classes\"></a>\n# Define useful classes\n[Back to Table of Contents](#toc)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://www.kaggle.com/timesler/facial-recognition-model-in-pytorch\nclass DetectionPipeline:\n    \"\"\"Pipeline class for detecting faces in the frames of a video file.\"\"\"\n    \n    def __init__(self, detector, n_frames=None, batch_size=60, resize=None):\n        \"\"\"Constructor for DetectionPipeline class.\n        \n        Keyword Arguments:\n            n_frames {int} -- Total number of frames to load. These will be evenly spaced\n                throughout the video. If not specified (i.e., None), all frames will be loaded.\n                (default: {None})\n            batch_size {int} -- Batch size to use with MTCNN face detector. (default: {32})\n            resize {float} -- Fraction by which to resize frames from original prior to face\n                detection. A value less than 1 results in downsampling and a value greater than\n                1 result in upsampling. (default: {None})\n        \"\"\"\n        self.detector = detector\n        self.n_frames = n_frames\n        self.batch_size = batch_size\n        self.resize = resize\n    \n    def __call__(self, filename):\n        \"\"\"Load frames from an MP4 video and detect faces.\n\n        Arguments:\n            filename {str} -- Path to video.\n        \"\"\"\n        # Create video reader and find length\n        v_cap = cv2.VideoCapture(filename)\n        v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        # Pick 'n_frames' evenly spaced frames to sample\n        if self.n_frames is None:\n            sample = np.arange(0, v_len)\n        else:\n            sample = np.linspace(0, v_len - 1, self.n_frames).astype(int)\n\n        # Loop through frames\n        faces = []\n        frames = []\n        for j in range(v_len):\n            success = v_cap.grab()\n            if j in sample:\n                # Load frame\n                success, frame = v_cap.retrieve()\n                if not success:\n                    continue\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                frame = Image.fromarray(frame)\n                \n                # Resize frame to desired size\n                if self.resize is not None:\n                    frame = frame.resize([int(d * self.resize) for d in frame.size])\n                frames.append(frame)\n\n                # When batch is full, detect faces and reset frame list\n                if len(frames) % self.batch_size == 0 or j == sample[-1]:\n                    faces.extend(self.detector(frames))\n                    frames = []\n\n        v_cap.release()\n\n        return faces\n    \n\nclass LogisticRegression(nn.Module):\n    def __init__(self, D_in=1, D_out=1):\n        super(LogisticRegression, self).__init__()\n        self.linear = nn.Linear(D_in, D_out)\n        \n    def forward(self, x):\n        y_pred = self.linear(x)\n        y_pred = torch.sigmoid(y_pred)\n        \n        return y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"define_helper_functions\"></a>\n# Define helper-functions\n[Back to Table of Contents](#toc)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://www.kaggle.com/timesler/facial-recognition-model-in-pytorch\ndef process_faces(faces, feature_extractor):\n    # Filter out frames without faces\n    faces = [f for f in faces if f is not None]\n    if len(faces) == 0:\n        return None\n    faces = torch.cat(faces).to(device)\n\n    # Generate facial feature vectors using a pretrained model\n    embeddings = feature_extractor(faces)\n\n    # Calculate centroid for video and distance of each face's feature vector from centroid\n    centroid = embeddings.mean(dim=0)\n    x = (embeddings - centroid).norm(dim=1).cpu().numpy()\n    \n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"start_inference_process\"></a>\n# Start inference process\n[Back to Table of Contents](#toc)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load model.\nclassifier = LogisticRegression()\nclassifier.load_state_dict(torch.load(MODEL_PATH))\nclassifier.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get all test videos.\nall_test_videos = glob.glob(os.path.join(TEST_DIR, '*.mp4'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load face detector.\nface_detector = MTCNN(margin=14, keep_all=True, factor=0.5, device=device).eval()\n\n# Load facial recognition model.\nfeature_extractor = InceptionResnetV1(pretrained='vggface2', device=device).eval()\n\n# Define face detection pipeline.\ndetection_pipeline = DetectionPipeline(detector=face_detector, n_frames=N_FRAMES, batch_size=BATCH_SIZE, resize=SCALE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = []\n\nwith torch.no_grad():\n    for path in tqdm(all_test_videos):\n        try:\n            # Detect all faces occur in the video.\n            faces = detection_pipeline(path)\n\n            # Calculate the distances of all faces' feature vectors to the centroid.\n            distances = process_faces(faces, feature_extractor)\n            X_test.append(distances)\n        except:\n            X_test.append(None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = []\n\nwith torch.no_grad():\n    for path, distances in zip(all_test_videos, X_test):\n        file_name = os.path.basename(path)\n\n        if distances is not None:\n            distances = torch.tensor(distances).unsqueeze(dim=1).float().to(device)\n            y_pred = classifier(distances)\n            y_pred = float(y_pred.mean().cpu().numpy())\n        else:\n            y_pred = DEFAULT_PROB\n\n        submission.append([file_name, y_pred])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"create_submission_csv\"></a>\n# Create submission.csv\n[Back to Table of Contents](#toc)"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(submission, columns=['filename', 'label'])\nsubmission.sort_values('filename').to_csv('submission.csv', index=False)\n\nplt.hist(submission.label, 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"conclusion\"></a>\n# Conclusion\nFinally, we made it! Let's submit the result to see whether we can get a better position in the Public Leaderboard =]]\n\nIf you have any questions or suggestions, feel free to move to the `comments` section below.\n\nPlease upvote this kernel if you think it is worth reading; and remember to upvote `@timesler`'s [*kernel*](https://www.kaggle.com/timesler/facial-recognition-model-in-pytorch), too. Thank you so much!\n\n[Back to Table of Contents](#toc)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}