{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Reference\n",
    " \n",
    " @author: [Hieu Phung](https://www.kaggle.com/phunghieu)\n",
    " \n",
    " @Link_Original:https://www.kaggle.com/phunghieu/deepfake-detection-inference-baseline\n",
    " \n",
    " @Goal: Studying the kernel and create my version in Tensorflow\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "# Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "1. [Configure hyper-parameters](#configure_hyper_parameters)\n",
    "1. [Install dependencies](#install_dependencies)\n",
    "1. [Import libraries](#import_libraries)\n",
    "1. [Define useful classes](#define_useful_classes)\n",
    "1. [Define helper-functions](#define_helper_functions)\n",
    "1. [Start inference process](#start_inference_process)\n",
    "1. [Create submission.csv](#create_submission_csv)\n",
    "1. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "# Introduction\n",
    "Let's use our trained Logistic Regression model in the previous step to infer all test videos, and submit the result to see how our simple model perform :)\n",
    "\n",
    "---\n",
    "Baseline's diagram\n",
    "![diagram](https://drive.google.com/uc?id=1vfKBChg33xHE1v5hRr-fP3ixxtlQjDc4&export=download)\n",
    "\n",
    "---\n",
    "This end-to-end solution includes 3 steps:\n",
    "1. [*Data Preparation* ](https://www.kaggle.com/phunghieu/deepfake-detection-data-preparation-baseline)\n",
    "1. [*Training*](https://www.kaggle.com/phunghieu/deepfake-detection-training-baseline)\n",
    "1. *Inference* <- **you're here**\n",
    "\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"configure_hyper_parameters\"></a>\n",
    "# Configure hyper-parameters\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "TEST_DIR = '/kaggle/input/deepfake-detection-challenge/test_videos/'\n",
    "MODEL_PATH = '/kaggle/input/deepfake-detection-logistic-regression/model.pth'\n",
    "\n",
    "BATCH_SIZE = 60\n",
    "SCALE = 0.25\n",
    "N_FRAMES = 10\n",
    "DEFAULT_PROB = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install_dependencies\"></a>\n",
    "# Install dependencies\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Install facenet-pytorch\n",
    "!pip install /kaggle/input/facenet-pytorch-vggface2/facenet_pytorch-2.0.0-py3-none-any.whl\n",
    "\n",
    "from facenet_pytorch.models.inception_resnet_v1 import get_torch_home\n",
    "torch_home = get_torch_home()\n",
    "\n",
    "# Copy model checkpoints to torch cache so they are loaded automatically by the package\n",
    "!mkdir -p $torch_home/checkpoints/\n",
    "!cp /kaggle/input/facenet-pytorch-vggface2/20180402-114759-vggface2-logits.pth $torch_home/checkpoints/vggface2_DG3kwML46X.pt\n",
    "!cp /kaggle/input/facenet-pytorch-vggface2/20180402-114759-vggface2-features.pth $torch_home/checkpoints/vggface2_G5aNV2VSMn.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"import_libraries\"></a>\n",
    "# Import libraries\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# See github.com/timesler/facenet-pytorch:\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1, extract_face\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'Running on device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"define_useful_classes\"></a>\n",
    "# Define useful classes\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/timesler/facial-recognition-model-in-pytorch\n",
    "class DetectionPipeline:\n",
    "    \"\"\"Pipeline class for detecting faces in the frames of a video file.\"\"\"\n",
    "    \n",
    "    def __init__(self, detector, n_frames=None, batch_size=60, resize=None):\n",
    "        \"\"\"Constructor for DetectionPipeline class.\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            n_frames {int} -- Total number of frames to load. These will be evenly spaced\n",
    "                throughout the video. If not specified (i.e., None), all frames will be loaded.\n",
    "                (default: {None})\n",
    "            batch_size {int} -- Batch size to use with MTCNN face detector. (default: {32})\n",
    "            resize {float} -- Fraction by which to resize frames from original prior to face\n",
    "                detection. A value less than 1 results in downsampling and a value greater than\n",
    "                1 result in upsampling. (default: {None})\n",
    "        \"\"\"\n",
    "        self.detector = detector\n",
    "        self.n_frames = n_frames\n",
    "        self.batch_size = batch_size\n",
    "        self.resize = resize\n",
    "    \n",
    "    def __call__(self, filename):\n",
    "        \"\"\"Load frames from an MP4 video and detect faces.\n",
    "\n",
    "        Arguments:\n",
    "            filename {str} -- Path to video.\n",
    "        \"\"\"\n",
    "        # Create video reader and find length\n",
    "        v_cap = cv2.VideoCapture(filename)\n",
    "        v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Pick 'n_frames' evenly spaced frames to sample\n",
    "        if self.n_frames is None:\n",
    "            sample = np.arange(0, v_len)\n",
    "        else:\n",
    "            sample = np.linspace(0, v_len - 1, self.n_frames).astype(int)\n",
    "\n",
    "        # Loop through frames\n",
    "        faces = []\n",
    "        frames = []\n",
    "        for j in range(v_len):\n",
    "            success = v_cap.grab()\n",
    "            if j in sample:\n",
    "                # Load frame\n",
    "                success, frame = v_cap.retrieve()\n",
    "                if not success:\n",
    "                    continue\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = Image.fromarray(frame)\n",
    "                \n",
    "                # Resize frame to desired size\n",
    "                if self.resize is not None:\n",
    "                    frame = frame.resize([int(d * self.resize) for d in frame.size])\n",
    "                frames.append(frame)\n",
    "\n",
    "                # When batch is full, detect faces and reset frame list\n",
    "                if len(frames) % self.batch_size == 0 or j == sample[-1]:\n",
    "                    faces.extend(self.detector(frames))\n",
    "                    frames = []\n",
    "\n",
    "        v_cap.release()\n",
    "\n",
    "        return faces\n",
    "    \n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, D_in=1, D_out=1):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(D_in, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"define_helper_functions\"></a>\n",
    "# Define helper-functions\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/timesler/facial-recognition-model-in-pytorch\n",
    "def process_faces(faces, feature_extractor):\n",
    "    # Filter out frames without faces\n",
    "    faces = [f for f in faces if f is not None]\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    faces = torch.cat(faces).to(device)\n",
    "\n",
    "    # Generate facial feature vectors using a pretrained model\n",
    "    embeddings = feature_extractor(faces)\n",
    "\n",
    "    # Calculate centroid for video and distance of each face's feature vector from centroid\n",
    "    centroid = embeddings.mean(dim=0)\n",
    "    x = (embeddings - centroid).norm(dim=1).cpu().numpy()\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"start_inference_process\"></a>\n",
    "# Start inference process\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model.\n",
    "classifier = LogisticRegression()\n",
    "classifier.load_state_dict(torch.load(MODEL_PATH))\n",
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all test videos.\n",
    "all_test_videos = glob.glob(os.path.join(TEST_DIR, '*.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load face detector.\n",
    "face_detector = MTCNN(margin=14, keep_all=True, factor=0.5, device=device).eval()\n",
    "\n",
    "# Load facial recognition model.\n",
    "feature_extractor = InceptionResnetV1(pretrained='vggface2', device=device).eval()\n",
    "\n",
    "# Define face detection pipeline.\n",
    "detection_pipeline = DetectionPipeline(detector=face_detector, n_frames=N_FRAMES, batch_size=BATCH_SIZE, resize=SCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for path in tqdm(all_test_videos):\n",
    "        try:\n",
    "            # Detect all faces occur in the video.\n",
    "            faces = detection_pipeline(path)\n",
    "\n",
    "            # Calculate the distances of all faces' feature vectors to the centroid.\n",
    "            distances = process_faces(faces, feature_extractor)\n",
    "            X_test.append(distances)\n",
    "        except:\n",
    "            X_test.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for path, distances in zip(all_test_videos, X_test):\n",
    "        file_name = os.path.basename(path)\n",
    "\n",
    "        if distances is not None:\n",
    "            distances = torch.tensor(distances).unsqueeze(dim=1).float().to(device)\n",
    "            y_pred = classifier(distances)\n",
    "            y_pred = float(y_pred.mean().cpu().numpy())\n",
    "        else:\n",
    "            y_pred = DEFAULT_PROB\n",
    "\n",
    "        submission.append([file_name, y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create_submission_csv\"></a>\n",
    "# Create submission.csv\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(submission, columns=['filename', 'label'])\n",
    "submission.sort_values('filename').to_csv('submission.csv', index=False)\n",
    "\n",
    "plt.hist(submission.label, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "# Conclusion\n",
    "Finally, we made it! Let's submit the result to see whether we can get a better position in the Public Leaderboard =]]\n",
    "\n",
    "If you have any questions or suggestions, feel free to move to the `comments` section below.\n",
    "\n",
    "Please upvote this kernel if you think it is worth reading; and remember to upvote `@timesler`'s [*kernel*](https://www.kaggle.com/timesler/facial-recognition-model-in-pytorch), too. Thank you so much!\n",
    "\n",
    "[Back to Table of Contents](#toc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
